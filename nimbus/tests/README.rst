
======================
NIMBUS tests directory
======================

This is an explanation of the NIMBUS tests in this directory, their
variations, and how to run them.

------------------------
Generating Documentation
------------------------

The source documentation uses ReStructured Text formatting.  It can be
converted to HTML like so::

    rst2html README.rst README.html

Or using scons::

    scons -u html

The source documentation for Sphinx is in the `doc/source` directory, and
the html target generates full documentation in HTML which can be browsed
from this file:

  doc/html/index.html


-------------
Running Tests
-------------

There are two types of tests: a unit tests executable ``nimbus_tests``
using the `Google unit testing framework
<https://github.com/google/googletest>`_, and a series of test cases for
comparing the output from the production and development versions of
``nimbus``.

All of the tests are automated by targets in the ``SConscript`` file.  They
can be selected individually or run all at once, and some of them can also
be run under valgrind for extended diagnostics.

The unit tests alone can be run with the *test* alias::

  scons -u test

The comparison tests can be run separately using the alias
*<project>-xdiff*, or all of them can be run with just *xdiff*::

  scons -u winter-xdiff
  scons -u ideas-xdiff
  scons -u xdiff
  
All of the tests, both unit tests and output comparison tests, can be run
with a single alias *xtest*::

  scons -u xtest

The tests depend upon some important SCons configuration variables,
particularly the comparison tests.  The nimbus production version is
located using the *JLOCAL_PROD* variable setting.  Set *JLOCAL_PROD* to the
RAF installation prefix, then the production NIMBUS path will be
``JLOCAL_PROD/bin/nimbus``.  This should be distinct from the *JLOCAL*
variable specifying where nimbus would be installed, otherwise the
production nimbus might be inadvertently overwritten by the development
version, making the comparison tests useless.  If NIMBUS should be built
against a different NIDAS installation, such as a version under
development, then that path can be specified in *NIDAS_PATH*.

Below is an example ``config.py`` file that would be written in the nimbus
``SConstruct`` directory::

  NIDAS_PATH="/opt/local/nidas"
  JLOCAL_PROD="/opt/local/raf"
  JLOCAL="/opt/local/raf-dev"

Unfortunately, running nimbus in batch mode still requires a X11 display.
For interactive console development, scons can pass the DISPLAY environment
variable to the nimbus processes.  However, this does not work for remote
shell sessions or real batch testing, so a virtual X11 server is used
instead.  The eol_scons xvfb tool runs Xvfb and sets the DISPLAY
environment variable so nimbus can connect to it.

----------
Unit Tests
----------

The unit tests are divided into these source files.

^^^^^^^^^^^^^^^^^^^^^^^^
``test_Interpolator.cc``
^^^^^^^^^^^^^^^^^^^^^^^^

Test the ``Interpolator`` class which wraps the GSL spline routines to
handle extrapolation cases.

^^^^^^^^^^^^^^^^^^^^^^^^
``test_parseInt.cc``
^^^^^^^^^^^^^^^^^^^^^^^^

Test the ``parseInt()`` function used to extract latch times from variable
list files.

^^^^^^^^^^^^^^^^^^^^^^^^
``test_sync_reader.cc``
^^^^^^^^^^^^^^^^^^^^^^^^

This is the real bulk of the unit testing, since it tests the sync records
interface provided by the ``sync_reader.cc`` module.

There are two *NidasProject* tests: *SerialNumbers* checks that the correct
serial numbers can be retrieved from the nidas xml file, while
*AircraftTest* tests the ``Aircraft::getAircraft()`` API added to NIDAS.

Then there are two *SyncReaderTest* tests: *CheckCurrentCalCoefficients*
and *CompareHeaderAndRecords*.  These tests work by starting up a sync
reader and then checking for expected values in the data and metadata (such
as calibration coefficients).

The *CompareHeaderAndRecords* test is the most complicated.  It compares
the header and the records from a sync record stream against a pre-recorded
sync record stream.  The known good sync record stream is generated from
``sync_dump`` and saved as JSON using the ``-j`` option.  This JSON stream
can then be read back in by the test and compared against the sync record
stream generated by NIMBUS.

NIMBUS reads sync records either from a DSM server in real-time or from a
raw data file in post-processing, so the *SyncReaderTest* tests are
instantiated for both cases.  The real-time test runs ``sync_server`` to
generate sync records, and then NIMBUS reads them from a pipe.  This is not
quite equivalent to real-time.  In real-time on the aircraft server, the
sync records are streamed over the network to localhost port 30001 instead
of through a pipe.  Also, the sorter lengths are much shorter in real-time.
To simulate real-time sorter lengths with ``sync_server``, these
command-line options need to be added: ``--procsorterlength 1.0
--rawsorterlength 0.25``.  However, that would complicate the tests because
there would be two different pre-recorded record streams, since the
different sorter lengths changes the values in the records.  Perhaps that
is something that should be handled more accurately someday.

The post-processing test instances use the SyncServer embedded in NIMBUS
rather than a separate sync_server process.

The JSON archive can be updated using ``scons -u updatetestdata``.


---------------------
Test Case Comparisons
---------------------

The SConscript includes test cases for running NIMBUS, each with a project,
flight number, raw data file, and start and end times.  Then for each
project, a copy of the project's configuration directory is included under
the ``projects`` subdirectory.  The configuration directories are actually
branches of the production configurations, so the test configurations can
be more stable and change only in step with the tests themselves.

The comparisons work by creating NIMBUS setup files for each case for each
NIMBUS version, one an installed production version and one the development
version in the current source tree.  The setup files specify a different
netcdf output file for each, then the scons builders can compare the netcdf
files with ``nc_compare``.  If both versions of NIMBUS produce identical
netcdf output, then the test passes.

Before the enhancements to ``nc_compare``, the netcdf files dumped to text
with ``ncdump -p 6`` and then compared with ``diff``.  The ``-p`` option
specifies the number of significant digits to be printed for floating point
numbers.  Using ``nc_compare`` allows for more careful comparison of
floating point values and can give better diagnostics when there are
differences.

Obviously the comparisons are not meaningful unless the production version
of NIMBUS is known to be correct and distinct from the development version.

----------------
SCons Tools
----------------


^^^^^^^^^^^^^^^^
NimbusSetup
^^^^^^^^^^^^^^^^

The python module ``NimbusSetup.py`` defines the class ``NimbusSetup`` for
creating and manipulating NIMBUS setup files.  It does not depend upon
SCons at all, and it has its own python tests which can be run like so::

  py.test NimbusSetup.py


^^^^^^^^^^^^^^^^
nimbus.py
^^^^^^^^^^^^^^^^

This is a scons tool which integrates the NimbusSetup class and other
NIMBUS builders into the SCons Environment.  The ``BatchNIMBUS()`` method
can be used to create a builder to run NIMBUS against an instance of a
NimbusSetup configuration, with or without the valgrind integration.  It
also defines helpers for comparing NIMBUS netcdf output.

^^^^^^^^^^^^^^^^
xvfb
^^^^^^^^^^^^^^^^

This tool runs Xvfb around nimbus actions, so that nimbus can connect to an
X server.  Unfortunately, a different Xvfb process is started for each
nimbus run, but the overhead probably is negligible.


----------------
Raw Data Cache
----------------

Rather than keep large raw data files in the repository, the SConscript
tests use the ``datafilecache`` tool to download raw data files and
maintain them in a cache directory.  By default this cache directory is in
the top-level SConstruct directory and called ``DataCache``, but alternate
locations can be added to the cache path if the files have been downloaded
elsewhere already.

When the tests run, if the raw data file does not already exist on the
cache path, then scons attempts to download the raw data file into the
first directory on the cache path which exists.  So for example, the
``#/DataCache`` default will not created or used at all if
``~/Data/raf/Raw_Data`` already exists.  The downloads use a ssh host alias
called *rafdata*, so that alias must exist and refer to an EOL host like
*barolo*, and obviously the host must be reachable, ie, no firewalls in the
way.

The cache can be refreshed at any time with the *datasync* alias and the
*download* option.  The *datasync* alias just makes sure all the cached
data files exist and downloads them if not, while the *download* option can
be used to force synchronization even if the data file already exists in
the cache::

  scons -u datasync download=force

If the raw data file changes, then any NIMBUS builders which use it will
then be outdated.

Note that synchronizing the data files can take several minutes even if
they are already updated locally, because rsync computes checksums on both
sides to verify the local copy is correct.


----------------
Valgrind
----------------

Several of the scons targets are instrumented using the ``valgrind`` tool.
There are aliases to run ``nimbus_tests`` with either the memcheck tool or
the helgrind threading tool::

  scons -u memcheck
  scons -u threadcheck

The valgrind output is written into a corresponding log file, such as
``memcheck.vg.log``, and the builder parses the log file and fails if there
are errors or definite memory leaks.

The test case comparisons can run valgrind on the development NIMBUS
version, but valgrind is off by default::

  scons -u winter-xdiff valgrind=on

In the example above, the valgrind output is written to
``WINTER/WINTER_rf03_actual.vg.log``.


------------------------
Comparing Whole Projects
------------------------

The NIMBUS tests directory also contains a python script called
`pnimbus.py` which can be used to run NIMBUS on a whole project's worth of
raw data, and then compare the netcdf output against some known good
output.

^^^^^^^^^^^^^^^
Usage
^^^^^^^^^^^^^^^

Pass the `--help` option to see all the usage info::

    Usage: pnimbus.py [options] {operation ...} [flight-spec ...]

    The named operations will be run for all the flights with setup files which
    match the flight specifiers listed after the operations.

    Modified setup files, netcdf output files, and log files will be written to
    the output directory named by the --output option.  The default output
    directory is a subdirectory named after the project.

    The flight specifiers have the form <project>/<profile>/<flight>, where any
    of the fields can be glob patterns.  If there is no /, then the specifier
    is just a project name and selects all the flights and profiles in that
    project.  The project must be found as a subdirectory of the PROJ_DIR
    variable.  An empty string on left side of the slash selects all the
    projects, and an empty string after the slash selects all the flights.  The
    profile field can be matched also, using the names mentioned below.  So a
    specifier like //rf01 selects the first research flight of all the known
    projects and profiles.

    Two profiles are created automatically for each project: base and local.
    The local profile runs the nimbus program within the local source tree.
    The base profile runs the installed nimbus program and writes output to a
    subdirectory of the local profile called BASE.  For the operations which
    compare the outputs of a project, the local profiles will be compared
    against the output of the corresponding base profiles.  In effect, the
    comparison operations only run on the 'local' profiles and not on the
    'base' profiles.  The command below only compares the local run with the
    base run.

      pnimbus.py compare CONTRAST//rf01

    Even though CONTRAST//rf01 matches both local and base profiles, the base
    profile does not have anything to compare against.

    These are the available operations:

      nimbus:

	Run NIMBUS on the setup files in the given production directory but
	with the output redirected to the specified output directory.

      reorder:

	Run just ncReorder on the output files.  The 'nimbus' operation runs
	ncReorder automatically when the nimbus program succeeds.

      process2d:

	Run process2d for the project flights, looking for the 2d input file
	corresponding to the flight from the setup file.

      compare: 

	Compare the netcdf file in the output directory with the output
	specified in the original setup files, using nc_compare.

      diff:

	Run diff on the nimbus output logs.  This is not necessarily possible
	except when comparing against a base project (see --base), since
	otherwise there are no log files.  The log files are preprocessed to
	remove timestamps from the log messages.

      ncdiff:

	Run diff on the ncdump output of the netcdf output files.  The diff
	command adds options to ignore the typical differences, and the ncdump
	prints floating point numbers with 6 significant digits.


    Options:
      -h, --help         show this help message and exit
      --debug            
      --info             
      --error            
      --flights          Ignore the operations and just list the flights selected by
			 the specifiers.
      --dryrun           Load configuration and echo steps but do not run any
			 commands.
      --output=OUTPUT    Set output directory for netcdf files and modified setup
			 files. Defaults to project name.
      --compare=COMPARE  Look for primary netcdf files in this directory, to be
			 compared against the netcdf files in the output directory.
      --nimbus=NIMBUS    Alternate path to nimbus executable.


^^^^^^^^^^^^^^^^^^^^^^
Specifying the project
^^^^^^^^^^^^^^^^^^^^^^

The `pnimbus.py` script needs to know which projects to process and where
to find the raw data files.  Typically the raw data files are located by
discovering all the setup files in each project's Production directory, and
those in turn can be discovered by looking up the project name under the
PROJ_DIR environment setting.  The examples below use the 'flights'
operation to just verify that the configuration loads correctly.  We don't
need to list all the profiles for each project, so the specifier matches
only the 'local' profiles:

      barolo|139|% echo $PROJ_DIR
      /net/jlocal/projects
      barolo|140|% ./pnimbus.py --flights CSET/local CSET/local/ff01 CSET/local/rf01 CSET/local/rf02 CSET/local/rf03 CSET/local/rf04 CSET/local/rf05 CSET/local/rf06 CSET/local/rf07 CSET/local/rf08 CSET/local/rf09 CSET/local/rf10 CSET/local/rf11 CSET/local/rf12 CSET/local/rf13 CSET/local/rf14 CSET/local/rf15 CSET/local/rf16

When no flight specifiers are given, the script uses a hardcoded list of
default projects.  You can see the list of available default projects using
the `--flights` option and matching a single flight:

      ./pnimbus.py --flights nimbus '//rf01'

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Generating output for a project
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

With the project configuration found and loaded, `pnimbus.py` can be used
to run nimbus processing steps on any or all of the flights in a project,
with the output being directed either to the current directory or to a
directory named with the `--output` option.

For each flight, `pnimbus` writes a modified setup file to the output
directory and then runs `nimbus` in the background with that setup file.
To run NIMBUS in the background, it must connect to a separate Xvfb session
started and stopped by `pnimbus`.  The standard output from NIMBUS is
logged to a file in the output directory named after the setup file being
run.

If nimbus exits without an error, then `ncReorder` is run automatically on
the output file.

This example runs NIMBUS on the CSET project, but only for flight `rf10`::

     ./pnimbus.py --info nimbus CSET/local/rf10

This command runs both base and local profiles for the same flight and then
compares the output::

     ./pnimbus.py --info nimbus compare CSET//rf10

^^^^^^^^^^^^^^^^^^^^^^^^^^
Comparing project output
^^^^^^^^^^^^^^^^^^^^^^^^^^

The output generated by the `pnimbus.py` script can be compared against
other nimbus runs, either production data in the standard locations or
output data generated by the `pnimbus.py` script with alternate versions of
nimbus.

The output files are compared using the `nc_compare` tool.  The
`nc_compare` tool will show statistical differences between variables as
well as differences in the set of variables, their attributes, and the
global attributes.



^^^^^^^^^^^^^^^^^^^^^^^^
Creating baseline output
^^^^^^^^^^^^^^^^^^^^^^^^

`pnimbus.py` can be used to create a baseline dataset manually, rather than
relying on existing production data.  This is useful when the available
production data have not been updated recently, so they are out of sync
with the "production" nimbus.  The raw data can be reprocessed with the
production nimbus to create the baseline output files, against which the
output from a development version (the "local" profile) of nimbus can be
compared.

This example runs nimbus for both the baseline and development project
configurations to generate the nimbus output files.  The "production"
nimbus is whatever is found on the PATH, while the development nimbus is
specified with the `--nimbus` option.

The 'base' profile conveniently specifies an output direcory which is
derived from the corresponding 'local' output directory.  If the local
project output directory is `CSET`, then the baseline output directory is
`CSET/BASE`.  The base and local project profiles can always be run
separately by matching only those specifiers on the command-line
specifiers.  For example, these commands create the baseline and local
output separately, then compare them::

      pnimbus.py nimbus WINTER/base/rf09
      pnimbus.py nimbus WINTER/local/rf09
      pnimbus.py compare WINTER/local/rf09

This command runs the above three steps one after the other::

      pnimbus.py nimbus compare WINTER//rf09

